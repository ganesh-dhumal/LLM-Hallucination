# LLM-Hallucination
LLM hallucinations are when language models produce incorrect, nonsensical, or unrelated outputs. These can range from minor mistakes to major fabrications, causing user confusion and giving a false impression of the model's understanding and accuracy.
